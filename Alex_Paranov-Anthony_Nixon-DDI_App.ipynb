{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT Term Project - DDI Classifier\n",
    "## Alex Paranov, Anthony Nixon\n",
    "### MIRI Masters - Term 2 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining Xml Classes\n",
    "\n",
    "The first component of our project is to create data structures in which we can store and manipulate the xml data in an effecient manner.\n",
    "\n",
    "We create four classes which correspond to the tagged elements in the xml annotation:\n",
    "\n",
    "#### Document:\n",
    "Represents and stores a full text sample consisting of sentence objects. The document class also contains a function \"set_features()\" which passes a call to a set_features() method at the sentence level and assigns featured words to the document featured_words list.\n",
    "\n",
    "#### Sentence:\n",
    "A sentence is a discrete segment of text which can be broken down into entities and pairs. The composing entities and pairs are stored in lists in the object of the same name.\n",
    "\n",
    "An important part of the Sentence class is it's \"set_features()\" function which iterates over the entities and splits the words in the text and tags them. Then for each tagged_word, the helper function \"get_featured_tuple()\" returns a list of features based on orthographic features, prefix and suffix, word shapes, etc. (We will cover these features and their rationale in more detail later in the report).\n",
    "\n",
    "#### Entity:\n",
    "Stores a relevent mention of a drug name / substance / etc. in a sentence as well as the location offset.\n",
    "\n",
    "#### Pair:\n",
    "A pair is a drug drug interation relating entities in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "from numpy import isfinite\n",
    "import re\n",
    "import string\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.sentences = []\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"DOCUMENT. Id: \"+self.id + '\\n'\n",
    "        for sentence in self.sentences:\n",
    "            st = st + sentence.__str__() + '\\n'\n",
    "        return st\n",
    "\n",
    "    # Sets features for each sentence\n",
    "    def set_features(self):\n",
    "        featured_words = []\n",
    "        featured_words_dict = [] #we need dictionary for preprocessing for ML algorithm\n",
    "        for sentence in self.sentences:\n",
    "            sent_features = sentence.set_features()\n",
    "            m_dict = {}\n",
    "            for s_feature in sent_features:\n",
    "                for i in range(len(s_feature)):\n",
    "                    m_dict[str(i)] = s_feature[i]\n",
    "\n",
    "                featured_words_dict.append(m_dict)\n",
    "            featured_words.extend(sent_features)\n",
    "\n",
    "        self.featured_words = featured_words\n",
    "        self.featured_words_dict = featured_words_dict\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, id, text):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.entities = []\n",
    "        self.pairs = []\n",
    "\n",
    "    def add_entity(self, entity):\n",
    "        self.entities.append(entity)\n",
    "\n",
    "    def add_pair(self, pair):\n",
    "        self.pairs.append(pair)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t---SENTENCE. Id: \"+self.id+\", Text: \"+self.text + '\\n'\n",
    "        for entity in self.entities:\n",
    "            st = st + entity.__str__() +'\\n'\n",
    "        return st\n",
    "\n",
    "    def set_features(self):\n",
    "        B_tags = [] #list with words that are of type B tag\n",
    "        I_tags = [] #list of words that are of type I tag\n",
    "        for entity in self.entities:\n",
    "            words = entity.text.split(\" \") #split words in text to tag\n",
    "            for index, word in enumerate(words):\n",
    "                if index == 0:\n",
    "                    B_tags.append(word)\n",
    "                else:\n",
    "                    I_tags.append(word)\n",
    "\n",
    "        tagged_words = pos_tag(word_tokenize(self.text))\n",
    "        all_features = []\n",
    "\n",
    "        for index, tagged_word in enumerate(tagged_words):\n",
    "            # We don't want to save punctuations\n",
    "            if len(tagged_word[0]) < 2:\n",
    "                continue\n",
    "            if tagged_word[0] in B_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'B'))\n",
    "            elif tagged_word[0] in I_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'I'))\n",
    "            else:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'O'))\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    # Following some guidelines from this table https://www.hindawi.com/journals/cmmm/2015/913489/tab1/\n",
    "    def get_featured_tuple(self, index, tagged_words, bio_tag):\n",
    "        features = [bio_tag]\n",
    "        word = tagged_words[index][0]\n",
    "\n",
    "        # get array of [word,pos_tag] for +-2 word window\n",
    "        if len(tagged_words) > 2:\n",
    "            windows = get_words_window(index, tagged_words, 2)\n",
    "        elif len(tagged_words) > 1:\n",
    "            windows = get_words_window(index, tagged_words, 1)\n",
    "        else:\n",
    "            windows = get_words_window(index, tagged_words, 0)\n",
    "\n",
    "        features.extend(windows)\n",
    "\n",
    "        # add length of a word\n",
    "        features.append(len(word))\n",
    "\n",
    "        orthographical_feature = get_orthographical_feature(word)\n",
    "        features.append(orthographical_feature)\n",
    "\n",
    "        # Prefix and suffixe is of lengths 3,4,5 respectively\n",
    "        prefix_suffix_features = get_prefix_suffix_feature(word)\n",
    "        features.extend(prefix_suffix_features)\n",
    "\n",
    "        # General word shape and brief word shape\n",
    "        word_shapes = get_word_shapes(word)\n",
    "        features.extend(word_shapes)\n",
    "\n",
    "        # May be add Y,N if drug is in drugbank or FDA approved list of drugs?\n",
    "        return tuple(features)\n",
    "\n",
    "# Getting words and pos tags of window +/- n\n",
    "# return will be [word-n,pos_tag-n,.....word+n,pos_tag+n]\n",
    "def get_words_window(index, tagged_words, n):\n",
    "    windows = []\n",
    "    if n >= len(tagged_words):\n",
    "        raise ValueError(\"n must be less than length of tagged_words\")\n",
    "\n",
    "    for i in range(-(n+1),n+1):\n",
    "        # we can reach the first and last element, so we are safe to get them\n",
    "        if index + i >= 0 and index + i < len(tagged_words):\n",
    "            word = tagged_words[index + i][0]\n",
    "            pos_tag = tagged_words[index + i][1]\n",
    "        else:\n",
    "            word = ''\n",
    "            pos_tag = ''\n",
    "\n",
    "        windows.append(word)\n",
    "        windows.append(pos_tag)\n",
    "\n",
    "    return windows\n",
    "\n",
    "def get_orthographical_feature(word):\n",
    "    orthographical_feature = \"alphanumeric\"\n",
    "    f_uppercase = lambda w: 1 if ord(w) >= 65 and ord(w) <= 90 else 0\n",
    "    upper_case = list(map(f_uppercase, word))\n",
    "\n",
    "    if sum(upper_case) == len(word):\n",
    "        orthographical_feature = \"all-capitalized\"\n",
    "    elif f_uppercase(word[0]) == 1:\n",
    "        orthographical_feature = \"is-capitalized\"\n",
    "\n",
    "    # Lambda function which uses ascii code of a character\n",
    "    f_numerics = lambda w: 1 if w.isnumeric() else 0\n",
    "    numerics = list(map(f_numerics, word))\n",
    "\n",
    "    if sum(numerics) == len(word):\n",
    "        orthographical_feature = \"all-digits\"\n",
    "\n",
    "    if \"-\" in word:\n",
    "        orthographical_feature += \"Y\"\n",
    "    else:\n",
    "        orthographical_feature += \"N\"\n",
    "\n",
    "    return orthographical_feature\n",
    "\n",
    "def get_prefix_suffix_feature(word):\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_word = snowball_stemmer.stem(word)\n",
    "    ind = word.find(stemmed_word)\n",
    "\n",
    "    prefix_len = len(word[:ind])\n",
    "    suffix_len = len(word) - prefix_len - len(stemmed_word)\n",
    "\n",
    "    pl3 = int(prefix_len == 3); sufl3 = int(suffix_len == 3)\n",
    "    pl4 = int(prefix_len == 4); sufl4 = int(suffix_len == 4)\n",
    "    pl5 = int(prefix_len == 5); sufl5 = int(suffix_len == 5)\n",
    "\n",
    "    return (pl3, pl4, pl5, sufl3, sufl4, sufl5)\n",
    "\n",
    "def get_word_shapes(word):\n",
    "    # Generalized Word Shape Feature. Map upper case, lower case, digit and\n",
    "    # other characters to X,x,0 and O respectively\n",
    "    # Aspirin1+ will be mapped to Xxxxxxx0O, for example\n",
    "    word_shape = \"\"\n",
    "    for w in word:\n",
    "        if w.isupper():\n",
    "            word_shape += \"X\"\n",
    "        elif w.islower():\n",
    "            word_shape += \"x\"\n",
    "        elif w.isnumeric():\n",
    "            word_shape += \"0\"\n",
    "        else:\n",
    "            word_shape += \"O\"\n",
    "\n",
    "    # Brief word shape. maps consecutive uppercase letters, lowercase letters,\n",
    "    # digits, and other characters to “X,” “x,” “0,” and “O,” respectively.\n",
    "    # Aspirin1+ will be mapped to Xx0O\n",
    "\n",
    "    # Lambda function to determine if character belongs to category other based on its ascii value\n",
    "    # We assume ascii unicode, which is true since our XML has UTF-8 encoding (English text)\n",
    "    f_other = lambda w: True if (ord(w) < 48 or (ord(w) >= 58 and ord(w) <= 64) or\n",
    "    (ord(w) >= 91 and ord(w) <= 96) or ord(w) > 122) else False\n",
    "\n",
    "    word_shape_brief = \"\"\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i].isupper():\n",
    "            word_shape_brief += \"X\"\n",
    "            while i < len(word) and word[i].isupper():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].islower():\n",
    "            word_shape_brief += \"x\"\n",
    "            while i < len(word) and word[i].islower():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].isnumeric():\n",
    "            word_shape_brief += \"0\"\n",
    "            while i < len(word) and word[i].isnumeric():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if f_other(word[i]):\n",
    "            word_shape_brief += \"O\"\n",
    "            while i < len(word) and f_other(word[i]):\n",
    "                i += 1\n",
    "                if i == len(word):\n",
    "                    break\n",
    "        i += 1\n",
    "\n",
    "    return (word_shape, word_shape_brief)\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, charOffset, type, text):\n",
    "        self.id = id\n",
    "        self.charOffset = charOffset\n",
    "        self.type = type\n",
    "        self.text = text\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---ENTITY. Id: \"+self.id+\", CharOffSet: \"+self.charOffset+\", Type: \"+self.type+\", Text: \"+self.text\n",
    "        return st\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self, id, e1, e2, ddi):\n",
    "        self.id = id\n",
    "        self.e1 = e1\n",
    "        self.e2 = e2\n",
    "        self.ddi = ddi\n",
    "        self.type = \"\"\n",
    "\n",
    "    def set_type(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---PAIR. Id: \"+self.id+\", E1: \"+self.e1+\", E2: \"+self.e2+\", DDI: \"+str(self.ddi)\n",
    "        if self.ddi:\n",
    "            st += \", Type: \"+self.type\n",
    "        return st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing\n",
    "\n",
    "The following code is our parser. The primary execution of the block is initiated by the parse_all_files() method. The parser first looks to see if the files have been parsed and stored locally, if not, then it will begin parsing.\n",
    "\n",
    "The parser stores the data in our Document, Sentence, Entity, and Pair objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "from xml_classes import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from os.path import abspath, join, isdir, exists\n",
    "from os import listdir, makedirs\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# Each dictionary contains name of dictionary and data, which is paths of all files in specified directory\n",
    "train_path = abspath(\"data/train/DrugBank\")\n",
    "drug_bank_train = {'name': 'drug_bank_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "train_path = abspath(\"data/train/MedLine\")\n",
    "medline_train =   {'name':'medline_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "# Test for DDI extraction task\n",
    "\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/DrugBank\")\n",
    "drug_bank_ddi_test = {'name': 'drug_bank_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/MedLine\")\n",
    "medline_ddi_test =   {'name': 'medline_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "# Test for DrugNER task\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/DrugBank\")\n",
    "drug_bank_ner_test = {'name': 'drug_bank_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/MedLine\")\n",
    "medline_ner_test =   {'name': 'medline_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "class Parser:\n",
    "    def set_path(self, xml_path):\n",
    "        self.path = xml_path\n",
    "\n",
    "    def parse_xml(self):\n",
    "        tree = ET.parse(self.path)\n",
    "        root = tree.getroot()\n",
    "        document = Document(root.attrib['id'])\n",
    "        for child in root:\n",
    "            if child.tag == \"sentence\":\n",
    "                sentence = Sentence(child.attrib['id'], child.attrib['text'])\n",
    "                if len(sentence.text) < 2:\n",
    "                    continue\n",
    "                for second_child in child:\n",
    "                    attr = second_child.attrib\n",
    "                    if second_child.tag == \"entity\":\n",
    "                        entity = Entity(attr['id'], attr['charOffset'], attr['type'], attr['text'])\n",
    "                        sentence.add_entity(entity)\n",
    "                    elif second_child.tag == \"pair\":\n",
    "                        ddi = False\n",
    "                        if attr['ddi'] == \"true\":\n",
    "                            ddi = True\n",
    "\n",
    "                        pair = Pair(attr['id'],attr['e1'],attr['e2'], ddi)\n",
    "                        if pair.ddi and 'type' in attr:\n",
    "                            pair.set_type(attr['type'])\n",
    "\n",
    "                        sentence.add_pair(pair)\n",
    "\n",
    "                document.add_sentence(sentence)\n",
    "        return document\n",
    "\n",
    "    def parse_save_xml_dict(self, xml_dict):\n",
    "        parsed_docs = []\n",
    "        for doc in xml_dict['data']:\n",
    "            print(\"Parsing: \"+doc)\n",
    "            self.set_path(doc)\n",
    "            d = self.parse_xml()\n",
    "            parsed_docs.append(d)\n",
    "\n",
    "        dir_path = abspath(\"data/pickle\")\n",
    "        if not isdir(dir_path):\n",
    "            makedirs(dir_path)\n",
    "\n",
    "        pickle_name = xml_dict['name']+\".pkl\"\n",
    "        with open(join(dir_path, pickle_name),\"wb\") as f:\n",
    "            pickle.dump(parsed_docs, f)\n",
    "            print(\"Saved parsed documents from \" + pickle_name + \" into pickle!\\n\")\n",
    "\n",
    "def parse_all_files():\n",
    "    parser = Parser()\n",
    "    if not exists(\"data/pickle/\"+drug_bank_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_train)\n",
    "    if not exists(\"data/pickle/\"+medline_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_train)\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ddi_test)\n",
    "    if not exists(\"data/pickle/\"+medline_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ddi_test)\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ner_test)\n",
    "    if not exists(\"data/pickle/\"+medline_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ner_test)\n",
    "\n",
    "def main():\n",
    "    parse_all_files()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
