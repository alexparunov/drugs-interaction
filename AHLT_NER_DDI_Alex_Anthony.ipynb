{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT Term Project - DDI Classifier\n",
    "## Alex Paranov, Anthony Nixon\n",
    "### MIRI Masters - Term 2 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining Python classes for XML processing\n",
    "\n",
    "The first component of our project is to create data structures in which we can store and manipulate the xml data in an efficient manner. This code is available in xml_classes.py (source available in appendix).\n",
    "\n",
    "We create four classes which correspond to the tagged elements in the xml annotation:\n",
    "\n",
    "#### Document:\n",
    "Represents and stores a full text sample consisting of sentence objects. The document class also contains a function \"set_features()\" which passes a call to a set_features() method at the sentence level and assigns featured words to the document featured_words list.\n",
    "\n",
    "#### Sentence:\n",
    "A sentence is a discrete segment of text which can be broken down into entities and pairs. The composing entities and pairs are stored in lists in the object of the same name.\n",
    "\n",
    "An important part of the Sentence class is it's \"set_features()\" function which, along with it’s helper functions, iterates over the entities and splits the words in the text and tags them. For each tagged_word the helper function \"get_featured_tuple()\" returns a list of features based on orthographic features, prefix and suffix, word shapes, etc. (We will cover these features and their rationale in more detail in the Part 3: feature extraction section of this report).\n",
    "\n",
    "#### Entity:\n",
    "Stores a relevent mention of a drug name / substance / etc. in a sentence as well as the location offset.\n",
    "\n",
    "#### Pair:\n",
    "A pair is a drug drug interation relating entities in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to import the classes\n",
    "from xml_classes import Document, Sentence, Entity, Pair\n",
    "print(\"DSEP classes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing\n",
    "\n",
    "After we have built the structures to store our data, we parse the data. Our parsing code is contained in parser.py (see appendix for source). The primary execution is initiated by the parse_all_files() method. The parser first looks to see if the project files have been previously parsed and stored locally, if not, then it will begin parsing.\n",
    "\n",
    "The parser stores the data in our Document, Sentence, Entity, and Pair objects. It then takes these objects and writes them to local disk as pickle files. In this manner, we only have to run the parser once.\n",
    "\n",
    "The data parsed is the drug_bank and med_line training sets as well as the drug-drug-interaction and name-entity-recognition test sets for both drug_bank and med_line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction\n",
    "\n",
    "The function extract_features() below loads the parsed pickle objects back into memory and calls set_features() on the documents.\n",
    "\n",
    "We follow an object oriented paradigm where each sentence object in a of the document object returns all features of its composing text (see appndix xml_classes.py for source).\n",
    "\n",
    "For our feature vectors we include the following features:\n",
    "\n",
    "- BIO (beginning, inside, outside) tag, which is tagged in each word.\n",
    "- Word windows consisting of [word, pos_tag] for +- n words. Which means if n is 2, we have word-2,pos-2,word-1,pos-1,word,pos,word+1,pos+1,word+2,pos+2\n",
    "- Boolean feature whether word is of length >= 7\n",
    "- Orthographical features: alphanumeric or all-capitalized, is-capitalized, all-digits, contains-hyphen Y or N.\n",
    "- Whether prefix and suffix of words are length 3, 4, or 5 respectively boolean e.g. [pl3, pl4, pl5, sufl3, sufl4, sufl5]\n",
    "- Word-shapes:\n",
    "    1. Generalized word shape, where the pattern of alphanumeric (X = upper, x = lower, 0 = numeric, O = other) eg. AAspirin11++ maps to XXxxxxxx0000.\n",
    "\n",
    "    2. Brief word shape, where consecutive forms aren’t condensed. eg. AAspirin11++ maps to Xx0O\n",
    " - Metadata. This last element of a word contains specific info of a drug, which will be used in formating predicted out. So each word contains sentenceId|offsets...|text|drug_type|sentenceId|idDrug1|idDrug2|prediction\n",
    "    First 4 elements are used in NER task and last 4 metadatas are used in DDI task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 4: Task 1 - Classification for Name-Entity-Recognition\n",
    "\n",
    "Our classification takes place through the Classifier class. The primary functions of the class are train_NER_model() and test_NER_model(), train_DDI_model() and test_DDI_model() which correspond to training and testing of both NER and DDI tasks.\n",
    "\n",
    "The function train_NER_model() or train_DDI_model() handles loading data, splitting into classes and vectorset, one-hot-encoding and training of a model which is either SVM or CRF. While functions test_NER_model() and test_DDI_model() takes care of loading a model from computer, predicting on test set, and outputing to the file which follows output format of task. Then this file can be used to test the performance of a model, i.e. accuracy recall and F-1 score.\n",
    "\n",
    "We ran many times both SVM and CRF models in order to tune parameters, which can be found in Appendix. However it is must to see that we used linear kernel in SVM since one-hot-encoding produces boolean and integer values.\n",
    "\n",
    "This is all done using our implemented CLI menu which is depicted below with instructions. So for example if we want to train a CRF model for NER task for drugbank and parse files before we run this command:\n",
    "\n",
    "./main -p -t 1 --train -f 1 -c 2\n",
    "\n",
    "This will train a CRF classifier and will save the model as ner_drugbankmodel_0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "usage: main.py [-h] [-p] [-t TASK] [--train] [--test] [-f FOLDER_INDEX]\n",
    "               [-i MODEL_INDEX] [-r RATIO] [-c CLASSIFIER]\n",
    "\n",
    "Train or Test model\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p, --parse           Parse files\n",
    "  -t TASK, --task TASK  Task of problem. 1 - NER task, 2 - DDI task.\n",
    "  --train               Train model\n",
    "  --test                Test model at index i\n",
    "  -f FOLDER_INDEX, --folder_index FOLDER_INDEX\n",
    "                        Folder number. 1 - drugbank, 2 - medline\n",
    "  -i MODEL_INDEX, --model_index MODEL_INDEX\n",
    "                        Index of a model to test\n",
    "  -r RATIO, --ratio RATIO\n",
    "                        Ratio of data to use for training\n",
    "  -c CLASSIFIER, --classifier CLASSIFIER\n",
    "                        Classifier to use. 1 - SVM, 2 - CRF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_bank_train objects already parsed - skipping\n",
      "medline_train objects already parsed - skipping\n",
      "drug_bank_ddi_test objects already parsed - skipping\n",
      "medline_ddi_test objects already parsed - skipping\n",
      "drug_bank_ner_test objects already parsed - skipping\n",
      "medline_ner_test objects already parsed - skipping\n",
      "All documents with features are set in /home/anthonyn/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/data/pickle/medline_ddi_test.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/main.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mparse_all_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/feature_extraction.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/feature_extraction.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mall_featured_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mall_featured_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/xml_classes.py\u001b[0m in \u001b[0;36mset_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfeatured_sent_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0msent_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms_feature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/xml_classes.py\u001b[0m in \u001b[0;36mset_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_featured_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_featured_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_metadatas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/xml_classes.py\u001b[0m in \u001b[0;36mget_featured_tuple\u001b[0;34m(self, index, tagged_words, bio_tag, window_size)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# get array of [word,pos_tag] for +-window_size word window. Default is 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_words_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/FIB Classes/AHLT/Term Project/drugs-interaction/xml_classes.py\u001b[0m in \u001b[0;36mget_words_window\u001b[0;34m(index, tagged_words, n)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n must be less than length of tagged_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;31m# we can reach the first and last element, so we are safe to get them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run main.py -p -t 1 --train -f 1 -c 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix - Source Code for Files:\n",
    "\n",
    "### xml_classes.py =========\n",
    "```python\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.sentences = []\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"DOCUMENT. Id: \"+self.id + '\\n'\n",
    "        for sentence in self.sentences:\n",
    "            st = st + sentence.__str__() + '\\n'\n",
    "        return st\n",
    "\n",
    "    # Sets features for each sentence\n",
    "    def set_features(self):\n",
    "        featured_words_dict = [] #we need dictionary for DictVectorizer\n",
    "        featured_sent_dict = []\n",
    "        for sentence in self.sentences:\n",
    "            sent_features = sentence.set_features()\n",
    "            sent_dict = []\n",
    "            for s_feature in sent_features:\n",
    "                # first indext contains BIO tag\n",
    "                # last index contains DDI bio tag\n",
    "                # previous to last index contains metadata\n",
    "                ddi_tag = s_feature.pop()\n",
    "                metadata = s_feature.pop()\n",
    "\n",
    "                assert isinstance(metadata, list)\n",
    "\n",
    "                m_dict = {'-2': metadata, '-1': ddi_tag}\n",
    "                for i in range(len(s_feature)):\n",
    "                    m_dict[str(i)] = s_feature[i]\n",
    "\n",
    "                featured_words_dict.append(m_dict)\n",
    "                sent_dict.append(m_dict)\n",
    "\n",
    "            featured_sent_dict.append(sent_dict)\n",
    "\n",
    "        self.featured_words_dict = featured_words_dict\n",
    "        self.featured_sent_dict = featured_sent_dict\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, id, text):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.entities = []\n",
    "        self.pairs = []\n",
    "\n",
    "    def add_entity(self, entity):\n",
    "        self.entities.append(entity)\n",
    "\n",
    "    def add_pair(self, pair):\n",
    "        self.pairs.append(pair)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t---SENTENCE. Id: \"+self.id+\", Text: \"+self.text + '\\n'\n",
    "        for entity in self.entities:\n",
    "            st = st + entity.__str__() +'\\n'\n",
    "        return st\n",
    "\n",
    "    def set_features(self):\n",
    "        B_tags = [] #list with words that are of type B tag\n",
    "        I_tags = [] #list of words that are of type I tag\n",
    "        for entity in self.entities:\n",
    "            words = entity.text.split(\" \") #split words in text to tag\n",
    "            for index, word in enumerate(words):\n",
    "                if index == 0:\n",
    "                    B_tags.append(word)\n",
    "                else:\n",
    "                    I_tags.append(word)\n",
    "\n",
    "        tagged_words = pos_tag(word_tokenize(self.text))\n",
    "        all_features = []\n",
    "\n",
    "        window_size = 2\n",
    "        for index, tagged_word in enumerate(tagged_words):\n",
    "            # We don't want to save punctuations\n",
    "            if len(tagged_word[0]) < 2:\n",
    "                continue\n",
    "            if tagged_word[0] in B_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'B', window_size))\n",
    "            elif tagged_word[0] in I_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'I', window_size))\n",
    "            else:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'O', window_size))\n",
    "\n",
    "        all_features = self.get_vector_metadatas(all_features, window_size)\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    # We need this loop in order to assign metadata to a drug-type word.\n",
    "    # It's necessary since our output should be of type:\n",
    "    # for NER task we need sentenceId|offsets...|text|type\n",
    "    # for DDI prediction we need sentenceId|idDrug1|idDrug2|prediction (ddi = 1 or ddi = 0)|type (advice, effect, etc.)\n",
    "    def get_vector_metadatas(self, all_features, window_size):\n",
    "        pos = 0 #initial search positions\n",
    "        new_all_features = [] #vector of new features with appended metadata\n",
    "        word_pos = 2 * window_size + 1 #position where main word is\n",
    "        for i in range(len(all_features)):\n",
    "            charOffset = \"\"\n",
    "            type = \"\" #type of drug which is empty by default\n",
    "            f_vector = all_features[i] #feature vector\n",
    "            if len(f_vector) <= word_pos:\n",
    "                continue\n",
    "            if isinstance(f_vector[len(f_vector)-1], list):\n",
    "                f_vector.pop()\n",
    "\n",
    "            f_word = str(f_vector[word_pos]) #word which is contained in postion 2*n+1\n",
    "            w_text = \"\" # word text\n",
    "            # if BIO tag of feature vector is B then we proceed with special case assignment\n",
    "            if f_vector[0] == 'B':\n",
    "                pos = self.text.find(f_word, pos) #find position where word starts in the sentence\n",
    "                # this should not be since there are always words in a sentence, but we don't want to deal with negative positions just in case\n",
    "                if pos < 0:\n",
    "                    continue\n",
    "\n",
    "                # beginning and end positions of word, so offset will be set accordingly\n",
    "                beg = pos; end = pos + len(f_word) - 1\n",
    "                charOffset = str(beg)+\"-\"+str(end)\n",
    "                pos = end #set a new search position to end of previous word, so that we search different words in sentence\n",
    "                w_text = f_word\n",
    "\n",
    "                metadata = [self.id, charOffset, w_text, type]\n",
    "                # appending metadata to last extracted feature vector (might be from inner while loop)\n",
    "                f_vector.append(metadata)\n",
    "                new_all_features.append(f_vector)\n",
    "\n",
    "                while i < len(all_features) - 1:\n",
    "                    f_vector = all_features[i+1] #next word in a feature vectors\n",
    "\n",
    "                    if len(f_vector) <= word_pos:\n",
    "                        continue\n",
    "\n",
    "                    if isinstance(f_vector[len(f_vector)-1], list):\n",
    "                        f_vector.pop()\n",
    "\n",
    "                    # As soon as next words BIO tag is not I, we break the inner loop\n",
    "                    # otherwise we continue appending to charOffsetString. So eventually it looks like\n",
    "                    # 100-150;155-170;190-200...\n",
    "                    if f_vector[0] != 'I':\n",
    "                        break\n",
    "\n",
    "                    f_word = str(f_vector[word_pos])\n",
    "                    pos = self.text.find(f_word, pos)\n",
    "\n",
    "                    if pos < 0:\n",
    "                        continue\n",
    "\n",
    "                    w_text += \" \"+ f_word\n",
    "                    beg = pos; end = pos + len(f_word) - 1\n",
    "                    charOffset += \";\" + str(beg)+\"-\"+str(end)\n",
    "                    pos = end\n",
    "                    i += 1\n",
    "\n",
    "                    metadata = [self.id, charOffset, w_text, type]\n",
    "                    # appending metadata to last extracted feature vector (might be from inner while loop)\n",
    "                    f_vector.append(metadata)\n",
    "                    new_all_features.append(f_vector)\n",
    "            else:\n",
    "                # Otherwise BIO tag is O so we simply have charOffset and empty type\n",
    "                f_word = str(f_vector[word_pos])\n",
    "                w_text = f_word\n",
    "                pos = self.text.find(f_word, pos)\n",
    "                if pos < 0:\n",
    "                    continue\n",
    "\n",
    "                beg = pos; end = pos + len(f_word) - 1\n",
    "                charOffset += str(beg)+\"-\"+str(end)\n",
    "                pos = end\n",
    "\n",
    "                metadata = [self.id, charOffset, w_text, type]\n",
    "                # appending metadata to last extracted feature vector (might be from inner while loop)\n",
    "                f_vector.append(metadata)\n",
    "                new_all_features.append(f_vector)\n",
    "\n",
    "        updated_features = []\n",
    "        for f_vector in new_all_features:\n",
    "            # Update tags. It means each tag will be of type B_drug/B_group/I_drug/I_group/etc.\n",
    "            if not isinstance(f_vector[len(f_vector)-1], list):\n",
    "                continue\n",
    "\n",
    "            metadata = f_vector.pop()\n",
    "\n",
    "            word_ddi = self.get_word_ddi(str(f_vector[word_pos]))\n",
    "            metadata.extend(word_ddi)\n",
    "\n",
    "            assert len(metadata) == 8\n",
    "            # if ddi = True then it's 1, otherwise it's 0\n",
    "            ddi_tag = int(metadata[4])\n",
    "\n",
    "            # append type of interaction in both cases\n",
    "            if ddi_tag > 0:\n",
    "                ddi_tag = str(ddi_tag)+\"_\"+metadata[len(metadata)-1]\n",
    "            else:\n",
    "                ddi_tag = str(ddi_tag)+\"_null\"\n",
    "\n",
    "            # update metadata\n",
    "            f_vector.append(metadata)\n",
    "\n",
    "            # set class of ddi to the last element\n",
    "            f_vector.append(ddi_tag)\n",
    "\n",
    "            tag = f_vector[0]\n",
    "            if tag == 'B' or tag == 'I':\n",
    "                type = self.get_word_entity(str(f_vector[word_pos]))\n",
    "                tag = tag + \"_\"+type\n",
    "                f_vector[0] = tag\n",
    "\n",
    "            # remove words at those indexes. They are located at positions word_pos +/- 2*i where i is in interval [-window_size,window_size and i != 0]\n",
    "            skipping_indexes = [word_pos + 2*i for i in range(-window_size,window_size+1) if i != 0]\n",
    "            ff_vector = [f_vector[j] for j in range(len(f_vector)) if j not in skipping_indexes]\n",
    "            updated_features.append(ff_vector)\n",
    "\n",
    "        return updated_features\n",
    "\n",
    "    # since words is of type BI tag, then it must have type.\n",
    "    # So we search through all entities and if word is contained then we set type\n",
    "    # NOTE that all types of word in offsets like this 100-150;155-170;190-200 will be the same\n",
    "    def get_word_entity(self, f_word):\n",
    "        for entity in self.entities:\n",
    "            text_ar = entity.text.split()\n",
    "            if f_word in text_ar:\n",
    "                return entity.type\n",
    "\n",
    "    def get_word_ddi(self, f_word):\n",
    "        ddi = False\n",
    "        idDrug1 = \"\"\n",
    "        idDrug2 = \"\"\n",
    "        type = \"\"\n",
    "        for entity in self.entities:\n",
    "            text_ar = entity.text.split()\n",
    "            if f_word in text_ar:\n",
    "                for pair in self.pairs:\n",
    "                    if pair.e1 == entity.id or pair.e2 == entity.id:\n",
    "                        ddi = pair.ddi\n",
    "                        idDrug1 = pair.e1\n",
    "                        idDrug2 = pair.e2\n",
    "                        type = pair.type\n",
    "\n",
    "        return [ddi, idDrug1, idDrug2, type]\n",
    "\n",
    "    # Following some guidelines from this table https://www.hindawi.com/journals/cmmm/2015/913489/tab1/\n",
    "    def get_featured_tuple(self, index, tagged_words, bio_tag, window_size = 2):\n",
    "        features = [bio_tag]\n",
    "        word = tagged_words[index][0]\n",
    "\n",
    "        # get array of [word,pos_tag] for +-window_size word window. Default is 2\n",
    "        if len(tagged_words) > window_size:\n",
    "            windows = get_words_window(index, tagged_words, window_size)\n",
    "            features.extend(windows)\n",
    "\n",
    "        # add boolean as length is more >= 7\n",
    "        features.append(int(len(word) >= 7))\n",
    "\n",
    "        orthographical_feature = get_orthographical_feature(word)\n",
    "        features.append(orthographical_feature)\n",
    "\n",
    "        # Prefix and suffix is of lengths 3,4,5 respectively\n",
    "        prefix_suffix_features = get_prefix_suffix_feature(word)\n",
    "        features.extend(prefix_suffix_features)\n",
    "\n",
    "        # General word shape and brief word shape\n",
    "        word_shapes = get_word_shapes(word)\n",
    "        features.extend(word_shapes)\n",
    "\n",
    "        # May be add Y,N if drug is in drugbank or FDA approved list of drugs?\n",
    "        return features\n",
    "\n",
    "# Getting words and pos tags of window +/- n\n",
    "# return will be [word-n,pos_tag-n,.....word+n,pos_tag+n]\n",
    "def get_words_window(index, tagged_words, n):\n",
    "    windows = []\n",
    "    if n >= len(tagged_words):\n",
    "        raise ValueError(\"n must be less than length of tagged_words\")\n",
    "\n",
    "    for i in range(-n,n+1):\n",
    "        # we can reach the first and last element, so we are safe to get them\n",
    "        if index + i >= 0 and index + i < len(tagged_words):\n",
    "            word = tagged_words[index + i][0]\n",
    "            pos_tag = tagged_words[index + i][1]\n",
    "        else:\n",
    "            word = ''\n",
    "            pos_tag = ''\n",
    "\n",
    "        windows.append(word)\n",
    "        windows.append(pos_tag)\n",
    "    return windows\n",
    "\n",
    "def get_orthographical_feature(word):\n",
    "    orthographical_feature = \"alphanumeric\"\n",
    "    f_uppercase = lambda w: 1 if ord(w) >= 65 and ord(w) <= 90 else 0\n",
    "    upper_case = list(map(f_uppercase, word))\n",
    "\n",
    "    if sum(upper_case) == len(word):\n",
    "        orthographical_feature = \"all-capitalized\"\n",
    "    elif f_uppercase(word[0]) == 1:\n",
    "        orthographical_feature = \"is-capitalized\"\n",
    "\n",
    "    # Lambda function which uses ascii code of a character\n",
    "    f_numerics = lambda w: 1 if w.isnumeric() else 0\n",
    "    numerics = list(map(f_numerics, word))\n",
    "\n",
    "    if sum(numerics) == len(word):\n",
    "        orthographical_feature = \"all-digits\"\n",
    "\n",
    "    if \"-\" in word:\n",
    "        orthographical_feature += \"Y\"\n",
    "    else:\n",
    "        orthographical_feature += \"N\"\n",
    "\n",
    "    return orthographical_feature\n",
    "\n",
    "def get_prefix_suffix_feature(word):\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_word = snowball_stemmer.stem(word)\n",
    "    ind = word.find(stemmed_word)\n",
    "\n",
    "    prefix_len = len(word[:ind])\n",
    "    suffix_len = len(word) - prefix_len - len(stemmed_word)\n",
    "\n",
    "    pl3 = int(prefix_len == 3); sufl3 = int(suffix_len == 3)\n",
    "    pl4 = int(prefix_len == 4); sufl4 = int(suffix_len == 4)\n",
    "    pl5 = int(prefix_len == 5); sufl5 = int(suffix_len == 5)\n",
    "\n",
    "    return (pl3, pl4, pl5, sufl3, sufl4, sufl5)\n",
    "\n",
    "def get_word_shapes(word):\n",
    "    # Generalized Word Shape Feature. Map upper case, lower case, digit and\n",
    "    # other characters to X,x,0 and O respectively\n",
    "    # Aspirin1+ will be mapped to Xxxxxxx0O, for example\n",
    "    word_shape = \"\"\n",
    "    for w in word:\n",
    "        if w.isupper():\n",
    "            word_shape += \"X\"\n",
    "        elif w.islower():\n",
    "            word_shape += \"x\"\n",
    "        elif w.isnumeric():\n",
    "            word_shape += \"0\"\n",
    "        else:\n",
    "            word_shape += \"O\"\n",
    "\n",
    "    # Brief word shape. maps consecutive uppercase letters, lowercase letters,\n",
    "    # digits, and other characters to “X,” “x,” “0,” and “O,” respectively.\n",
    "    # Aspirin1+ will be mapped to Xx0O\n",
    "\n",
    "    # Lambda function to determine if character belongs to category other based on its ascii value\n",
    "    # We assume ascii unicode, which is true since our XML has UTF-8 encoding (English text)\n",
    "    f_other = lambda w: True if (ord(w) < 48 or (ord(w) >= 58 and ord(w) <= 64) or\n",
    "    (ord(w) >= 91 and ord(w) <= 96) or ord(w) > 122) else False\n",
    "\n",
    "    word_shape_brief = \"\"\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i].isupper():\n",
    "            word_shape_brief += \"X\"\n",
    "            while i < len(word) and word[i].isupper():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].islower():\n",
    "            word_shape_brief += \"x\"\n",
    "            while i < len(word) and word[i].islower():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].isnumeric():\n",
    "            word_shape_brief += \"0\"\n",
    "            while i < len(word) and word[i].isnumeric():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if f_other(word[i]):\n",
    "            word_shape_brief += \"O\"\n",
    "            while i < len(word) and f_other(word[i]):\n",
    "                i += 1\n",
    "                if i == len(word):\n",
    "                    break\n",
    "        i += 1\n",
    "\n",
    "    return (word_shape, word_shape_brief)\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, charOffset, type, text):\n",
    "        self.id = id\n",
    "        self.charOffset = charOffset\n",
    "        self.type = type\n",
    "        self.text = text\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---ENTITY. Id: \"+self.id+\", CharOffSet: \"+self.charOffset+\", Type: \"+self.type+\", Text: \"+self.text\n",
    "        return st\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self, id, e1, e2, ddi):\n",
    "        self.id = id\n",
    "        self.e1 = e1\n",
    "        self.e2 = e2\n",
    "        self.ddi = ddi\n",
    "        self.type = \"\"\n",
    "\n",
    "    def set_type(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---PAIR. Id: \"+self.id+\", E1: \"+self.e1+\", E2: \"+self.e2+\", DDI: \"+str(self.ddi)\n",
    "        if self.ddi:\n",
    "            st += \", Type: \"+self.type\n",
    "        return st\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parser.py =====\n",
    "```python\n",
    "#!/usr/bin/python3\n",
    "from xml_classes import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from os.path import abspath, join, isdir, exists\n",
    "from os import listdir, makedirs\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# Each dictionary contains name of dictionary and data, which is paths of all files in specified directory\n",
    "train_path = abspath(\"data/train/DrugBank\")\n",
    "drug_bank_train = {'name': 'drug_bank_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "train_path = abspath(\"data/train/MedLine\")\n",
    "medline_train =   {'name':'medline_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "# Test for DDI extraction task\n",
    "\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/DrugBank\")\n",
    "drug_bank_ddi_test = {'name': 'drug_bank_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/MedLine\")\n",
    "medline_ddi_test =   {'name': 'medline_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "# Test for DrugNER task\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/DrugBank\")\n",
    "drug_bank_ner_test = {'name': 'drug_bank_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/MedLine\")\n",
    "medline_ner_test =   {'name': 'medline_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "class Parser:\n",
    "    def set_path(self, xml_path):\n",
    "        self.path = xml_path\n",
    "\n",
    "    def parse_xml(self):\n",
    "        tree = ET.parse(self.path)\n",
    "        root = tree.getroot()\n",
    "        document = Document(root.attrib['id'])\n",
    "        for child in root:\n",
    "            if child.tag == \"sentence\":\n",
    "                sentence = Sentence(child.attrib['id'], child.attrib['text'])\n",
    "                if len(sentence.text) < 2:\n",
    "                    continue\n",
    "                for second_child in child:\n",
    "                    attr = second_child.attrib\n",
    "                    if second_child.tag == \"entity\":\n",
    "                        entity = Entity(attr['id'], attr['charOffset'], attr['type'], attr['text'])\n",
    "                        sentence.add_entity(entity)\n",
    "                    elif second_child.tag == \"pair\":\n",
    "                        ddi = False\n",
    "                        if attr['ddi'] == \"true\":\n",
    "                            ddi = True\n",
    "\n",
    "                        pair = Pair(attr['id'],attr['e1'],attr['e2'], ddi)\n",
    "                        if pair.ddi and 'type' in attr:\n",
    "                            pair.set_type(attr['type'])\n",
    "\n",
    "                        sentence.add_pair(pair)\n",
    "\n",
    "                document.add_sentence(sentence)\n",
    "        return document\n",
    "\n",
    "    def parse_save_xml_dict(self, xml_dict):\n",
    "        parsed_docs = []\n",
    "        for doc in xml_dict['data']:\n",
    "            print(\"Parsing: \"+doc)\n",
    "            self.set_path(doc)\n",
    "            d = self.parse_xml()\n",
    "            parsed_docs.append(d)\n",
    "\n",
    "        dir_path = abspath(\"data/pickle\")\n",
    "        if not isdir(dir_path):\n",
    "            makedirs(dir_path)\n",
    "\n",
    "        pickle_name = xml_dict['name']+\".pkl\"\n",
    "        with open(join(dir_path, pickle_name),\"wb\") as f:\n",
    "            pickle.dump(parsed_docs, f)\n",
    "            print(\"Saved parsed documents from \" + pickle_name + \" into pickle!\\n\")\n",
    "\n",
    "def parse_all_files():\n",
    "    parser = Parser()\n",
    "    if not exists(\"data/pickle/\"+drug_bank_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_train)\n",
    "    else:\n",
    "        print(\"drug_bank_train objects already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_train)\n",
    "    else:\n",
    "        print(\"medline_train objects already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ddi_test)\n",
    "    else:\n",
    "        print(\"drug_bank_ddi_test objects already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ddi_test)\n",
    "    else:\n",
    "        print(\"medline_ddi_test objects already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ner_test)\n",
    "    else:\n",
    "        print(\"drug_bank_ner_test objects already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ner_test)\n",
    "    else:\n",
    "        print(\"medline_ner_test objects already parsed - skipping\")\n",
    "\n",
    "def main():\n",
    "    parse_all_files()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier.py ===\n",
    "\n",
    "```python\n",
    "#!/usr/bin/python3\n",
    "import pickle\n",
    "from os.path import join, abspath, isdir\n",
    "from os import listdir, makedirs\n",
    "from operator import contains\n",
    "\n",
    "from numpy.random import randint\n",
    "import scipy\n",
    "\n",
    "import sklearn_crfsuite\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Files are in the following order (on Alex's computer):\n",
    "# 0 - medline_ner_test.pkl\n",
    "# 1 - medline_train.pkl\n",
    "# 2 - drug_bank_ddi_test.pkl\n",
    "# 3 - drug_bank_train.pkl\n",
    "# 4 - drug_bank_ner_test.pkl\n",
    "# 5 - medline_ddi_test.pkl\n",
    "\n",
    "pickle_path = \"data/pickle\"\n",
    "pickled_files = [join(abspath(pickle_path), f) for f in listdir(abspath(pickle_path))]\n",
    "\n",
    "# function that returns full path of a file\n",
    "def get_file_full_path(file_name, pickled_files):\n",
    "    for p_f in pickled_files:\n",
    "        if contains(p_f, file_name):\n",
    "            return p_f\n",
    "    return \"\"\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.path = \"\"\n",
    "\n",
    "    def set_path(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    # split dataset into classes and sub-dictionaries\n",
    "    # return classes and dictionaries (i.e. feature vectors)\n",
    "    def split_dataset(self):\n",
    "        if len(self.path) == 0:\n",
    "            raise ValueError(\"Path can't be empty\")\n",
    "\n",
    "        with open(self.path, 'rb') as f:\n",
    "            docs = pickle.load(f)\n",
    "\n",
    "        feature_vectors_dict = [] # feature vectors expressed as dicts. train data\n",
    "        ner_classes = [] # B,I,O classes\n",
    "        ddi_classes = [] # B,I,O classes\n",
    "        dict_metadatas = []\n",
    "\n",
    "        for doc in docs:\n",
    "            for m_dict in doc.featured_words_dict:\n",
    "                ner_classes.append(m_dict['0'])\n",
    "                ddi_classes.append(m_dict['-1'])\n",
    "                dict_metadatas.append(m_dict['-2'])\n",
    "\n",
    "                # we want sub-dictionary of all elements besides the class\n",
    "                sub_dict = {k:v for k,v in  m_dict.items() if k > '0' and not isinstance(v, list)}\n",
    "                feature_vectors_dict.append(sub_dict)\n",
    "\n",
    "        return (feature_vectors_dict, ner_classes, ddi_classes, dict_metadatas)\n",
    "\n",
    "    def split_dataset_crf(self):\n",
    "        if len(self.path) == 0:\n",
    "            raise ValueError(\"Path can't be empty\")\n",
    "\n",
    "        with open(self.path, 'rb') as f:\n",
    "            docs = pickle.load(f)\n",
    "\n",
    "        all_sentences_features = []\n",
    "        for doc in docs:\n",
    "            # sent is a list of dictionaries\n",
    "            for sent in doc.featured_sent_dict:\n",
    "                ner_classes = []\n",
    "                ddi_classes = []\n",
    "                dict_metadatas = []\n",
    "                sub_dicts = []\n",
    "                for m_dict in sent:\n",
    "                    ner_classes.append(m_dict['0'])\n",
    "                    ddi_classes.append(m_dict['-1'])\n",
    "                    dict_metadatas.append(m_dict['-2'])\n",
    "\n",
    "                    # we want sub-dictionary of all elements besides the class\n",
    "                    sub_dict = {k:v for k,v in  m_dict.items() if k > '0' and not isinstance(v, list)}\n",
    "                    sub_dicts.append(sub_dict)\n",
    "\n",
    "                all_sentences_features.append((sub_dicts, ner_classes, ddi_classes, dict_metadatas))\n",
    "\n",
    "        return all_sentences_features\n",
    "\n",
    "    # train dataset, where X is a list of feature vectors expressed as dictionary\n",
    "    # and Y is class variable, which is BIO tag_type in our case. ratio is proportion of data to use to train\n",
    "    def train_dataset_svm(self, X, Y, kernel, ratio):\n",
    "        vec = DictVectorizer(sparse=False)\n",
    "        svm_clf = svm.SVC(kernel = kernel, cache_size = 1800, C = 20, verbose = True, tol = 0.01)\n",
    "        vec_clf = Pipeline([('vectorizer', vec), ('svm', svm_clf)])\n",
    "        assert len(X) == len(Y)\n",
    "\n",
    "        # subset of indexes to used in training\n",
    "        r_indexes = randint(low = 0, high = len(X)-1, size = round(ratio*(len(X)-1)))\n",
    "\n",
    "        X_subset = [X[i] for i in r_indexes]\n",
    "        Y_subset = [Y[i] for i in r_indexes]\n",
    "\n",
    "        vec_clf.fit(X_subset, Y_subset)\n",
    "\n",
    "        return vec_clf\n",
    "\n",
    "    def train_dataset_crf(self, X, Y, ratio):\n",
    "        crf = sklearn_crfsuite.CRF(algorithm = 'lbfgs', max_iterations = 100, all_possible_transitions = True)\n",
    "\n",
    "        params_space = { 'c1': scipy.stats.expon(scale = 0.5), 'c2': scipy.stats.expon(scale = 0.05)}\n",
    "\n",
    "        import multiprocessing\n",
    "        cpus = multiprocessing.cpu_count()\n",
    "        rs = RandomizedSearchCV(crf, params_space, cv = 3, verbose = 1, n_jobs = cpus-1, n_iter = 50)\n",
    "\n",
    "        assert len(X) == len(Y)\n",
    "\n",
    "        # subset of indexes to used in training\n",
    "        r_indexes = randint(low = 0, high = len(X)-1, size = round(ratio*(len(X)-1)))\n",
    "\n",
    "        X_subset = [X[i] for i in r_indexes]\n",
    "        Y_subset = [Y[i] for i in r_indexes]\n",
    "\n",
    "        rs.fit(X_subset, Y_subset)\n",
    "\n",
    "        return rs\n",
    "\n",
    "    def train_NER_model(self, train_folder, kernel = 'linear', ratio = 1, classifier = 1):\n",
    "        if not isdir('models'):\n",
    "            makedirs('models')\n",
    "\n",
    "        model_name = \"\"\n",
    "        model_index = 0\n",
    "        model_names = [join(abspath(\"models\"), f) for f in listdir(abspath(\"models\"))]\n",
    "\n",
    "        if train_folder == 1:\n",
    "            path = get_file_full_path(\"drug_bank_train.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            drugbank_models = list(filter(lambda x: contains(x, 'drugbank_model_'), model_names))\n",
    "            model_index = len(drugbank_models) # save next model\n",
    "            model_name = 'models/ner_drugbank_model_'+str(model_index)+'.pkl'\n",
    "            print(\"Started training NER Drugbank model...\")\n",
    "        elif train_folder == 2:\n",
    "            path = get_file_full_path(\"medline_train.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            medline_models = list(filter(lambda x: contains(x, 'medline_model_'), model_names))\n",
    "            model_index = len(medline_models) # save next model\n",
    "            model_name = 'models/ner_medline_model_'+str(model_index)+'.pkl'\n",
    "            print(\"Started training NER Medline model...\")\n",
    "        else:\n",
    "            raise ValueError('train_folder value should be 1 - drugbank, or 2 - medline')\n",
    "\n",
    "        if classifier == 2:\n",
    "            featured_sent_dict = self.split_dataset_crf()\n",
    "            X_train = [f[0] for f in featured_sent_dict]\n",
    "            Y_train = [f[1] for f in featured_sent_dict]\n",
    "            clf = self.train_dataset_crf(X_train, Y_train, ratio)\n",
    "        else:\n",
    "            # we ignore Y_ddi classes since they are not used for NER model training\n",
    "            X_train, Y_train, Y_ddi, metadatas = self.split_dataset()\n",
    "            clf = self.train_dataset_svm(X_train, Y_train, kernel, ratio)\n",
    "\n",
    "        joblib.dump(clf, model_name)\n",
    "        print(\"\\nNER Model trained and saved into\", model_name)\n",
    "\n",
    "    def test_NER_model(self, model_index, test_folder, classifier = 1):\n",
    "        model_name = \"\"\n",
    "        predictions_name = \"\"\n",
    "        if test_folder == 1:\n",
    "            model_name = 'models/ner_drugbank_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/ner_drugbank_model_'+str(model_index)+'.txt'\n",
    "            path = get_file_full_path(\"drug_bank_ner_test.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            print(\"Testing NER Drugbank model\", model_index,\"...\")\n",
    "        elif test_folder == 2:\n",
    "            model_name = 'models/ner_medline_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/ner_medline_model_'+str(model_index)+'.txt'\n",
    "            path = get_file_full_path(\"medline_ner_test.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            print(\"Testing NER Medline model\", model_index,\"...\")\n",
    "        else:\n",
    "            raise ValueError('test_folder value should be 1 - drugbank, or 2 - medline')\n",
    "\n",
    "        vec_clf = joblib.load(model_name)\n",
    "\n",
    "        if classifier == 2:\n",
    "            featured_sent_dict = self.split_dataset_crf()\n",
    "            X_test = [f[0] for f in featured_sent_dict]\n",
    "            Y = [f[1] for f in featured_sent_dict] # ner classes\n",
    "            Y_test = []\n",
    "            for y in Y:\n",
    "                for y_test in y:\n",
    "                    Y_test.append(y_test)\n",
    "\n",
    "            met = [f[3] for f in featured_sent_dict]\n",
    "            metadatas = []\n",
    "            for metadata in met:\n",
    "                for met in metadata:\n",
    "                    metadatas.append(met)\n",
    "        else:\n",
    "            # metadatas are of type: sentenceId | offsets... | text | type\n",
    "            # we ignore Y_ddi classes since they are not used for NER model training\n",
    "            X_test, Y_test, Y_ddi, metadatas = self.split_dataset()\n",
    "\n",
    "        if classifier == 2:\n",
    "            preds = vec_clf.predict(X_test)\n",
    "            predictions = []\n",
    "            for pred in preds:\n",
    "                for prediction in pred:\n",
    "                    predictions.append(prediction)\n",
    "        else:\n",
    "            predictions = vec_clf.predict(X_test)\n",
    "\n",
    "        assert len(predictions) == len(Y_test) == len(metadatas)\n",
    "\n",
    "        if not isdir('predictions'):\n",
    "            makedirs('predictions')\n",
    "\n",
    "        pr_f = open(predictions_name,'w')\n",
    "        # clear file, i.e. remove all\n",
    "        pr_f.close()\n",
    "\n",
    "        # reopen clean file\n",
    "        pr_f = open(predictions_name, 'w')\n",
    "\n",
    "        for i, pred in enumerate(predictions):\n",
    "            metadata = metadatas[i]\n",
    "            # if prediction is B_type or I_type then we predicted the drug and it's type is after B_, thus we can write into check file\n",
    "            if pred[0] == 'B':\n",
    "                line = metadata[0] + '|' + metadata[1] + '|' + metadata[2] + '|' + pred[2:]\n",
    "                pr_f.write(line + '\\n')\n",
    "\n",
    "        print(\"\\nNER Predictions are saved in file\", predictions_name)\n",
    "        pr_f.close()\n",
    "\n",
    "    def train_DDI_model(self, train_folder, kernel = 'linear', ratio = 1, classifier = 1):\n",
    "        if not isdir('models'):\n",
    "            makedirs('models')\n",
    "\n",
    "        model_name = \"\"\n",
    "        model_index = 0\n",
    "        model_names = [join(abspath(\"models\"), f) for f in listdir(abspath(\"models\"))]\n",
    "        if train_folder == 1:\n",
    "            path = get_file_full_path(\"drug_bank_train.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            drugbank_ddi_models = list(filter(lambda x: contains(x, 'drugbank_ddi_model_'), model_names))\n",
    "            model_index = len(drugbank_ddi_models) # save next model\n",
    "            model_name = 'models/ddi_drugbank_model_'+str(model_index)+'.pkl'\n",
    "            print(\"Started training DDI Drugbank model...\")\n",
    "        elif train_folder == 2:\n",
    "            path = get_file_full_path(\"medline_train.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            medline_ddi_models = list(filter(lambda x: contains(x, 'medline_ddi_model_'), model_names))\n",
    "            model_index = len(medline_ddi_models) # save next model\n",
    "            model_name = 'models/ddi_medline_model_'+str(model_index)+'.pkl'\n",
    "            print(\"Started training DDI Medline model...\")\n",
    "        else:\n",
    "            raise ValueError('train_folder value should be 1 - drugbank, or 2 - medline')\n",
    "\n",
    "        X_train, Y_ner, Y_train, metadatas = self.split_dataset()\n",
    "\n",
    "        if classifier == 2:\n",
    "            featured_sent_dict = self.split_dataset_crf()\n",
    "            X_train = [f[0] for f in featured_sent_dict]\n",
    "            Y_train = [f[2] for f in featured_sent_dict] # ddi classes\n",
    "            clf = self.train_dataset_crf(X_train, Y_train, ratio)\n",
    "        else:\n",
    "            # we ignore Y_ddi classes since they are not used for NER model training\n",
    "            X_train, Y_train, Y_ddi, metadatas = self.split_dataset()\n",
    "            clf = self.train_dataset_svm(X_train, Y_train, kernel, ratio)\n",
    "\n",
    "        joblib.dump(clf, model_name)\n",
    "        print(\"\\nDDI Model trained and saved into\", model_name)\n",
    "\n",
    "    def test_DDI_model(self, model_index, test_folder, classifier):\n",
    "        model_name = \"\"\n",
    "        predictions_name = \"\"\n",
    "        if test_folder == 1:\n",
    "            model_name = 'models/ddi_drugbank_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/ddi_drugbank_model_'+str(model_index)+'.txt'\n",
    "            path = get_file_full_path(\"drug_bank_ddi_test.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            print(\"Testing DDI Drugbank model\", model_index,\"...\")\n",
    "        elif test_folder == 2:\n",
    "            model_name = 'models/ddi_medline_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/ddi_medline_model_'+str(model_index)+'.txt'\n",
    "            path = get_file_full_path(\"medline_ddi_test.pkl\", pickled_files)\n",
    "            self.set_path(path)\n",
    "            print(\"Testing DDI Medline model\", model_index,\"...\")\n",
    "        else:\n",
    "            raise ValueError('test_folder value should be 1 - drugbank, or 2 - medline')\n",
    "\n",
    "        vec_clf = joblib.load(model_name)\n",
    "\n",
    "        if classifier == 2:\n",
    "            featured_sent_dict = self.split_dataset_crf()\n",
    "            X_test = [f[0] for f in featured_sent_dict]\n",
    "            Y = [f[1] for f in featured_sent_dict]\n",
    "            Y_test = []\n",
    "            for y in Y:\n",
    "                for y_test in y:\n",
    "                    Y_test.append(y_test)\n",
    "\n",
    "            met = [f[3] for f in featured_sent_dict]\n",
    "            metadatas = []\n",
    "            for metadata in met:\n",
    "                for met in metadata:\n",
    "                    metadatas.append(met)\n",
    "        else:\n",
    "            # metadatas are of type: sentenceId | offsets... | text | type\n",
    "            # we ignore Y_ddi classes since they are not used for NER model training\n",
    "            X_test, Y_test, Y_ddi, metadatas = self.split_dataset()\n",
    "\n",
    "        if classifier == 2:\n",
    "            preds = vec_clf.predict(X_test)\n",
    "            predictions = []\n",
    "            for pred in preds:\n",
    "                for prediction in pred:\n",
    "                    predictions.append(prediction)\n",
    "        else:\n",
    "            predictions = vec_clf.predict(X_test)\n",
    "\n",
    "        assert len(predictions) == len(Y_test) == len(metadatas)\n",
    "\n",
    "        if not isdir('predictions'):\n",
    "            makedirs('predictions')\n",
    "\n",
    "        pr_f = open(predictions_name,'w')\n",
    "        # clear file, i.e. remove all\n",
    "        pr_f.close()\n",
    "\n",
    "        # reopen clean file\n",
    "        pr_f = open(predictions_name, 'w')\n",
    "\n",
    "        for i, pred in enumerate(predictions):\n",
    "            metadata = metadatas[i]\n",
    "            # if prediction is 1_type then we predicted ddi correctly, thus we can write into check file\n",
    "            if len(metadata[5]) > 0 and len(metadata[6]) > 0:\n",
    "                # some document sentence ids are completely wrong, we want only DDI-Medline\n",
    "                if metadata[0][:4] == 'DDI-':\n",
    "                    line = metadata[0]+'|'+metadata[5]+'|'+metadata[6]+'|'+pred[0]+'|'+pred[2:]\n",
    "                    pr_f.write(line + '\\n')\n",
    "\n",
    "        print(\"\\nDDIPredictions are saved in file\", predictions_name)\n",
    "        pr_f.close()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
