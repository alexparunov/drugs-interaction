{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT Term Project - DDI Classifier\n",
    "## Alex Paranov, Anthony Nixon\n",
    "### MIRI Masters - Term 2 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining Python classes for XML processing\n",
    "\n",
    "The first component of our project is to create data structures in which we can store and manipulate the xml data in an efficient manner. This code is available in xml_classes.py (source available in appendix).\n",
    "\n",
    "We create four classes which correspond to the tagged elements in the xml annotation:\n",
    "\n",
    "#### Document:\n",
    "Represents and stores a full text sample consisting of sentence objects. The document class also contains a function \"set_features()\" which passes a call to a set_features() method at the sentence level and assigns featured words to the document featured_words list.\n",
    "\n",
    "#### Sentence:\n",
    "A sentence is a discrete segment of text which can be broken down into entities and pairs. The composing entities and pairs are stored in lists in the object of the same name.\n",
    "\n",
    "An important part of the Sentence class is it's \"set_features()\" function which, along with it’s helper functions, iterates over the entities and splits the words in the text and tags them. For each tagged_word the helper function \"get_featured_tuple()\" returns a list of features based on orthographic features, prefix and suffix, word shapes, etc. (We will cover these features and their rationale in more detail in the Part 3: feature extraction section of this report).\n",
    "\n",
    "#### Entity:\n",
    "Stores a relevent mention of a drug name / substance / etc. in a sentence as well as the location offset.\n",
    "\n",
    "#### Pair:\n",
    "A pair is a drug drug interation relating entities in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to import the classes\n",
    "from xml_classes import Document, Sentence, Entity, Pair\n",
    "print(\"DSEP classes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing\n",
    "\n",
    "After we have built the structures to store our data, we parse the data. Our parsing code is contained in parser.py (see appendix for source). The primary execution is initiated by the parse_all_files() method. The parser first looks to see if the project files have been previously parsed and stored locally, if not, then it will begin parsing.\n",
    "\n",
    "The parser stores the data in our Document, Sentence, Entity, and Pair objects. It then takes these objects and writes them to local disk as pickle files. In this manner, we only have to run the parser once.\n",
    "\n",
    "The data parsed is the drug_bank and med_line training sets as well as the drug-drug-interaction and name-entity-recognition test sets for both drug_bank and med_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to begin parsing\n",
    "from parser import main as parse_all_files\n",
    "parse_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction\n",
    "\n",
    "The function extract_features() below loads the parsed pickle objects back into memory and calls set_features() on the documents.\n",
    "\n",
    "We follow an object oriented paradigm where each sentence object in a of the document object returns all features of its composing text (see appndix xml_classes.py for source).\n",
    "\n",
    "For our feature vectors we include the following features:\n",
    "\n",
    "# TODO: Enter the feature mathjax FORMULAS def as in Lab BELOW\n",
    "\n",
    "- BIO (beginning, inside, outside) tag\n",
    "- Word windows consisting of [word, pos_tag] for +- 2 words\n",
    "- Boolean of length >= 7\n",
    "- Orthographical features: alphanumeric, capitalization, digits, hyphenation\n",
    "- Whether prefix and suffix of words are length 3, 4, or 5 boolean e.g. [pl3, pl4, pl5, sufl3, sufl4, sufl5]\n",
    "- Word-shape:\n",
    "    1. Generalized word shape, where the pattern of alphanumeric (X = upper, x = lower, 0 = numeric, O = other) eg. Aspirin1+ maps to Xxxxxxx00.\n",
    "\n",
    "    2. Brief word shape, where consecutive forms aren’t condensed. eg. Aspirin1+ maps to Xx0O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cell to load documents from pickle file and extract features from\n",
    "# the sentences.\n",
    "\n",
    "from os.path import join, abspath\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "pickle_path = \"data/pickle\"\n",
    "pickled_files = [join(abspath(pickle_path), f) for f in listdir(abspath(pickle_path))]\n",
    "\n",
    "def extract_features():\n",
    "    for file_name in pickled_files:\n",
    "        f = open(file_name, 'rb')\n",
    "        docs = pickle.load(f)\n",
    "        f.close()\n",
    "        all_featured_docs = []\n",
    "        for doc in docs:\n",
    "            #print(\"Extracting features for\",doc.id)\n",
    "            doc.set_features()\n",
    "            all_featured_docs.append(doc)\n",
    "\n",
    "        with open(file_name,'wb') as f:\n",
    "            pickle.dump(all_featured_docs, f)\n",
    "            print(\"All documents with features are set in \"+file_name)\n",
    "\n",
    "extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 4: Task 1 - Classification for Name-Entity-Recognition\n",
    "\n",
    "Our classification takes place through the NERClassifier class. The primary functions of the class are train_drugbank() and test_NER_model() which correspond to training and testing of the name entity recognition task.\n",
    "\n",
    "The functions handle the loading of the data, one-hot-encoding, parameters and call to the SVM classifier and the result output.\n",
    "We experimented with both an RBF and linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [-h] [-p] [-t TASK] [--train] [--test] [-f FOLDER_INDEX]\n",
      "               [-i MODEL_INDEX] [-r RATIO] [-c CLASSIFIER]\n",
      "\n",
      "Train or Test model\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -p, --parse           Parse files\n",
      "  -t TASK, --task TASK  Task of problem. 1 - NER task, 2 - DDI task.\n",
      "  --train               Train model\n",
      "  --test                Test model at index i\n",
      "  -f FOLDER_INDEX, --folder_index FOLDER_INDEX\n",
      "                        Folder number. 1 - drugbank, 2 - medline\n",
      "  -i MODEL_INDEX, --model_index MODEL_INDEX\n",
      "                        Index of a model to test\n",
      "  -r RATIO, --ratio RATIO\n",
      "                        Ratio of data to use for training\n",
      "  -c CLASSIFIER, --classifier CLASSIFIER\n",
      "                        Classifier to use. 1 - SVM, 2 - CRF\n"
     ]
    }
   ],
   "source": [
    "# run cell to construct classifier\n",
    "\n",
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN A MODEL - based on feature vectures extracted in Part 3 - if you would like to use a pre-existing model, skip to next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supress scikit warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    nerCl.train_drugbank(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST A MODEL - run the cell below with relevant parameters, model_index (you will find this appended to the file name of the model) and test_folder (1 = drugbank, 2 = medline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerCl.test_NER_model(model_index = 2, test_folder = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Task 2 - Classification for Drug-Drug-Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Appendix - Source Code for Files:\n",
    "\n",
    "### xml_classes.py =========\n",
    "```python\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.sentences = []\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"DOCUMENT. Id: \"+self.id + '\\n'\n",
    "        for sentence in self.sentences:\n",
    "            st = st + sentence.__str__() + '\\n'\n",
    "        return st\n",
    "\n",
    "    # Sets features for each sentence\n",
    "    def set_features(self):\n",
    "        #we need dictionary for DictVectorizer\n",
    "        featured_words_dict = []\n",
    "        for sentence in self.sentences:\n",
    "            sent_features = sentence.set_features()\n",
    "            for s_feature in sent_features:\n",
    "\n",
    "                # first indext contains BIO tag\n",
    "                # last index contains metadata\n",
    "                m_dict = {'-1': s_feature[len(s_feature)-1] }\n",
    "                for i in range(len(s_feature) - 1):\n",
    "                    m_dict[str(i)] = s_feature[i]\n",
    "                featured_words_dict.append(m_dict)\n",
    "\n",
    "        self.featured_words_dict = featured_words_dict\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, id, text):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.entities = []\n",
    "        self.pairs = []\n",
    "\n",
    "    def add_entity(self, entity):\n",
    "        self.entities.append(entity)\n",
    "\n",
    "    def add_pair(self, pair):\n",
    "        self.pairs.append(pair)\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t---SENTENCE. Id: \"+self.id+\", Text: \"+self.text + '\\n'\n",
    "        for entity in self.entities:\n",
    "            st = st + entity.__str__() +'\\n'\n",
    "        return st\n",
    "\n",
    "    def set_features(self):\n",
    "        B_tags = [] #list with words that are of type B tag\n",
    "        I_tags = [] #list of words that are of type I tag\n",
    "        for entity in self.entities:\n",
    "            #split words in text to tag\n",
    "            words = entity.text.split(\" \")\n",
    "            for index, word in enumerate(words):\n",
    "                if index == 0:\n",
    "                    B_tags.append(word)\n",
    "                else:\n",
    "                    I_tags.append(word)\n",
    "\n",
    "        tagged_words = pos_tag(word_tokenize(self.text))\n",
    "        all_features = []\n",
    "\n",
    "        for index, tagged_word in enumerate(tagged_words):\n",
    "            # We don't want to save punctuations\n",
    "            if len(tagged_word[0]) < 2:\n",
    "                continue\n",
    "            if tagged_word[0] in B_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'B'))\n",
    "            elif tagged_word[0] in I_tags:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'I'))\n",
    "            else:\n",
    "                all_features.append(self.get_featured_tuple(index, tagged_words, 'O'))\n",
    "\n",
    "        all_features = self.get_vector_metadatas(all_features)\n",
    "\n",
    "        return all_features\n",
    "\n",
    "    # We need this loop in order to assign metadata to a \n",
    "    # drug-type word.\n",
    "    # It's necessary since our output should be of type:\n",
    "    # sentenceId|offsets...|text|type\n",
    "    def get_vector_metadatas(self, all_features):\n",
    "        pos = 0 #initial search positions\n",
    "        \n",
    "        #vector of new features with appended metadata  \n",
    "        new_all_features = []\n",
    "        for i in range(len(all_features)):\n",
    "            charOffset = \"\"\n",
    "            type = \"\" #type of drug which is empty by default\n",
    "            f_vector = all_features[i] #feature vector\n",
    "            #word which is contained in postion 3\n",
    "            f_word = str(f_vector[5])\n",
    "            w_text = \"\" # word text\n",
    "            # if BIO tag of feature vector is B then we \n",
    "            # proceed with special case assignment\n",
    "            if f_vector[0] == 'B':\n",
    "                #find position where word starts in the sentence\n",
    "                pos = self.text.find(f_word, pos)\n",
    "                # this should not be since there are always words \n",
    "                # in a sentence, but we don't want to deal with\n",
    "                # negative positions just in case\n",
    "                if pos < 0:\n",
    "                    continue\n",
    "\n",
    "                # beginning and end positions of word, so offset \n",
    "                # will be set accordingly\n",
    "                beg = pos; end = pos + len(f_word) - 1\n",
    "                charOffset = str(beg)+\"-\"+str(end)\n",
    "                \n",
    "                #set a new search position to end of previous word, \n",
    "                # so that we search different words in sentence \n",
    "                pos = end \n",
    "                w_text = f_word\n",
    "\n",
    "                metadata = [self.id, charOffset, w_text, type]\n",
    "                # appending metadata to last extracted feature vector\n",
    "                # (might be from inner while loop)\n",
    "                f_vector.append(metadata)\n",
    "                new_all_features.append(f_vector)\n",
    "\n",
    "                while i < len(all_features) - 1:\n",
    "                    #next word in a feature vectors\n",
    "                    f_vector = all_features[i+1]\n",
    "\n",
    "                    # As soon as next words BIO tag is not I, \n",
    "                    # we break the inner loop\n",
    "                    # otherwise we continue appending to \n",
    "                    # charOffsetString. \n",
    "                    # So eventually it looks like\n",
    "                    # 100-150;155-170;190-200...\n",
    "                    if f_vector[0] != 'I':\n",
    "                        break\n",
    "\n",
    "                    f_word = str(f_vector[5])\n",
    "                    pos = self.text.find(f_word, pos)\n",
    "\n",
    "                    if pos < 0:\n",
    "                        continue\n",
    "\n",
    "                    w_text += \" \"+ f_word\n",
    "                    beg = pos; end = pos + len(f_word) - 1\n",
    "                    charOffset += \";\" + str(beg)+\"-\"+str(end)\n",
    "                    pos = end\n",
    "                    i += 1\n",
    "\n",
    "                    metadata = [self.id, charOffset, w_text, type]\n",
    "                    # appending metadata to last extracted feature \n",
    "                    # vector (might be from inner while loop)\n",
    "                    f_vector.append(metadata)\n",
    "                    new_all_features.append(f_vector)\n",
    "            else:\n",
    "                # Otherwise BIO tag is O so we simply have \n",
    "                # charOffset and empty type\n",
    "                f_word = str(f_vector[5])\n",
    "                w_text = f_word\n",
    "                pos = self.text.find(f_word, pos)\n",
    "                if pos < 0:\n",
    "                    continue\n",
    "\n",
    "                beg = pos; end = pos + len(f_word) - 1\n",
    "                charOffset += str(beg)+\"-\"+str(end)\n",
    "                pos = end\n",
    "\n",
    "                metadata = [self.id, charOffset, w_text, type]\n",
    "                # appending metadata to last extracted feature \n",
    "                # vector (might be from inner while loop)\n",
    "                f_vector.append(metadata)\n",
    "                new_all_features.append(f_vector)\n",
    "\n",
    "        updated_features = []\n",
    "        for f_vector in new_all_features:\n",
    "            # Update tags. It means each tag will be of type \n",
    "            # B_drug/B_group/I_drug/I_group/etc.\n",
    "            try:\n",
    "                tag = f_vector[0]\n",
    "                if tag == 'B' or tag == 'I':\n",
    "                    type = self.get_word_entity(str(f_vector[5]))\n",
    "                    tag = tag + \"_\"+type\n",
    "                    f_vector[0] = tag\n",
    "\n",
    "                # remove words at windows. Words are located at\n",
    "                # positions 1,3,7,9 in window of n = 2\n",
    "                # We need to remove them otherwise training takes\n",
    "                # forever\n",
    "                ff_vector = [f_vector[j] for j in range(len(f_vector)) if j != 1 and j != 3 and j != 7 and j != 9]\n",
    "                updated_features.append(ff_vector)\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "        return updated_features\n",
    "\n",
    "    # since words is of type BI tag, then it must have type.\n",
    "    # So we search through all entities and if word is contained \n",
    "    # then we set type\n",
    "    # NOTE that all types of word in offsets like this \n",
    "    # 100-150;155-170;190-200 will be the same\n",
    "    def get_word_entity(self, f_word):\n",
    "        for entity in self.entities:\n",
    "            text_ar = entity.text.split()\n",
    "            if f_word in text_ar:\n",
    "                return entity.type\n",
    "\n",
    "    # Following some guidelines from this table https://www.hindawi.com/journals/cmmm/2015/913489/tab1/\n",
    "    def get_featured_tuple(self, index, tagged_words, bio_tag):\n",
    "        features = [bio_tag]\n",
    "        word = tagged_words[index][0]\n",
    "\n",
    "        # get array of [word,pos_tag] for +-2 word window\n",
    "        if len(tagged_words) > 2:\n",
    "            windows = get_words_window(index, tagged_words, 2)\n",
    "            features.extend(windows)\n",
    "\n",
    "        # add boolean as length is more >= 7\n",
    "        features.append(int(len(word) >= 7))\n",
    "\n",
    "        orthographical_feature = get_orthographical_feature(word)\n",
    "        features.append(orthographical_feature)\n",
    "\n",
    "        # Prefix and suffix is of lengths 3,4,5 respectively\n",
    "        prefix_suffix_features = get_prefix_suffix_feature(word)\n",
    "        features.extend(prefix_suffix_features)\n",
    "\n",
    "        # General word shape and brief word shape\n",
    "        word_shapes = get_word_shapes(word)\n",
    "        features.extend(word_shapes)\n",
    "\n",
    "        # May be add Y,N if drug is in drugbank or \n",
    "        # FDA approved list of drugs?\n",
    "        return features\n",
    "\n",
    "# Getting words and pos tags of window +/- n\n",
    "# return will be [word-n,pos_tag-n,.....word+n,pos_tag+n]\n",
    "def get_words_window(index, tagged_words, n):\n",
    "    windows = []\n",
    "    if n >= len(tagged_words):\n",
    "        raise ValueError(\"n must be less than length of tagged_words\")\n",
    "\n",
    "    for i in range(-n,n+1):\n",
    "        # we can reach the first and last element, \n",
    "        # so we are safe to get them\n",
    "        if index + i >= 0 and index + i < len(tagged_words):\n",
    "            word = tagged_words[index + i][0]\n",
    "            pos_tag = tagged_words[index + i][1]\n",
    "        else:\n",
    "            word = ''\n",
    "            pos_tag = ''\n",
    "\n",
    "        windows.append(word)\n",
    "        windows.append(pos_tag)\n",
    "    return windows\n",
    "\n",
    "def get_orthographical_feature(word):\n",
    "    orthographical_feature = \"alphanumeric\"\n",
    "    f_uppercase = lambda w: 1 if ord(w) >= 65 and ord(w) <= 90 else 0\n",
    "    upper_case = list(map(f_uppercase, word))\n",
    "\n",
    "    if sum(upper_case) == len(word):\n",
    "        orthographical_feature = \"all-capitalized\"\n",
    "    elif f_uppercase(word[0]) == 1:\n",
    "        orthographical_feature = \"is-capitalized\"\n",
    "\n",
    "    # Lambda function which uses ascii code of a character\n",
    "    f_numerics = lambda w: 1 if w.isnumeric() else 0\n",
    "    numerics = list(map(f_numerics, word))\n",
    "\n",
    "    if sum(numerics) == len(word):\n",
    "        orthographical_feature = \"all-digits\"\n",
    "\n",
    "    if \"-\" in word:\n",
    "        orthographical_feature += \"Y\"\n",
    "    else:\n",
    "        orthographical_feature += \"N\"\n",
    "\n",
    "    return orthographical_feature\n",
    "\n",
    "def get_prefix_suffix_feature(word):\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_word = snowball_stemmer.stem(word)\n",
    "    ind = word.find(stemmed_word)\n",
    "\n",
    "    prefix_len = len(word[:ind])\n",
    "    suffix_len = len(word) - prefix_len - len(stemmed_word)\n",
    "\n",
    "    pl3 = int(prefix_len == 3); sufl3 = int(suffix_len == 3)\n",
    "    pl4 = int(prefix_len == 4); sufl4 = int(suffix_len == 4)\n",
    "    pl5 = int(prefix_len == 5); sufl5 = int(suffix_len == 5)\n",
    "\n",
    "    return (pl3, pl4, pl5, sufl3, sufl4, sufl5)\n",
    "\n",
    "def get_word_shapes(word):\n",
    "    # Generalized Word Shape Feature. Map upper case, lower case, \n",
    "    # digit and other characters to X,x,0 and O respectively\n",
    "    # Aspirin1+ will be mapped to Xxxxxxx0O, for example\n",
    "    word_shape = \"\"\n",
    "    for w in word:\n",
    "        if w.isupper():\n",
    "            word_shape += \"X\"\n",
    "        elif w.islower():\n",
    "            word_shape += \"x\"\n",
    "        elif w.isnumeric():\n",
    "            word_shape += \"0\"\n",
    "        else:\n",
    "            word_shape += \"O\"\n",
    "\n",
    "    # Brief word shape. maps consecutive uppercase letters, \n",
    "    # lowercase letters, digits, and other characters \n",
    "    # to “X,” “x,” “0,” and “O,” respectively.\n",
    "    # Aspirin1+ will be mapped to Xx0O\n",
    "\n",
    "    # Lambda function to determine if character belongs to \n",
    "    # category other based on its ascii value\n",
    "    # We assume ascii unicode, which is true since our XML \n",
    "    # has UTF-8 encoding (English text)\n",
    "    f_other = lambda w: True if (ord(w) < 48 or (ord(w) >= 58 and ord(w) <= 64) or\n",
    "    (ord(w) >= 91 and ord(w) <= 96) or ord(w) > 122) else False\n",
    "\n",
    "    word_shape_brief = \"\"\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i].isupper():\n",
    "            word_shape_brief += \"X\"\n",
    "            while i < len(word) and word[i].isupper():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].islower():\n",
    "            word_shape_brief += \"x\"\n",
    "            while i < len(word) and word[i].islower():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if word[i].isnumeric():\n",
    "            word_shape_brief += \"0\"\n",
    "            while i < len(word) and word[i].isnumeric():\n",
    "                i += 1\n",
    "            if i == len(word):\n",
    "                break\n",
    "        if f_other(word[i]):\n",
    "            word_shape_brief += \"O\"\n",
    "            while i < len(word) and f_other(word[i]):\n",
    "                i += 1\n",
    "                if i == len(word):\n",
    "                    break\n",
    "        i += 1\n",
    "\n",
    "    return (word_shape, word_shape_brief)\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, id, charOffset, type, text):\n",
    "        self.id = id\n",
    "        self.charOffset = charOffset\n",
    "        self.type = type\n",
    "        self.text = text\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---ENTITY. Id: \"+self.id+\", CharOffSet: \"+self.charOffset+\", Type: \"+self.type+\", Text: \"+self.text\n",
    "        return st\n",
    "\n",
    "class Pair:\n",
    "    def __init__(self, id, e1, e2, ddi):\n",
    "        self.id = id\n",
    "        self.e1 = e1\n",
    "        self.e2 = e2\n",
    "        self.ddi = ddi\n",
    "        self.type = \"\"\n",
    "\n",
    "    def set_type(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        st = \"\\t\\t---PAIR. Id: \"+self.id+\", E1: \"+self.e1+\", E2: \"+self.e2+\", DDI: \"+str(self.ddi)\n",
    "        if self.ddi:\n",
    "            st += \", Type: \"+self.type\n",
    "        return st\n",
    "```\n",
    "\n",
    "### parser.py =========\n",
    "\n",
    "```python\n",
    "#!/usr/bin/python3\n",
    "from xml_classes import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from os.path import abspath, join, isdir, exists\n",
    "from os import listdir, makedirs\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# Each dictionary contains name of dictionary and data, \n",
    "# which is paths of all files in specified directory\n",
    "train_path = abspath(\"data/train/DrugBank\")\n",
    "drug_bank_train = {'name': 'drug_bank_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "train_path = abspath(\"data/train/MedLine\")\n",
    "medline_train =   {'name':'medline_train', 'data': [join(train_path, f) for f in listdir(train_path)]}\n",
    "\n",
    "# Test for DDI extraction task\n",
    "\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/DrugBank\")\n",
    "drug_bank_ddi_test = {'name': 'drug_bank_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DDI_Extraction_task/MedLine\")\n",
    "medline_ddi_test =   {'name': 'medline_ddi_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "# Test for DrugNER task\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/DrugBank\")\n",
    "drug_bank_ner_test = {'name': 'drug_bank_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "test_path = abspath(\"data/test/Test_DrugNER_task/MedLine\")\n",
    "medline_ner_test =   {'name': 'medline_ner_test', 'data': [join(test_path, f) for f in listdir(test_path)]}\n",
    "\n",
    "class Parser:\n",
    "    def set_path(self, xml_path):\n",
    "        self.path = xml_path\n",
    "\n",
    "    def parse_xml(self):\n",
    "        tree = ET.parse(self.path)\n",
    "        root = tree.getroot()\n",
    "        document = Document(root.attrib['id'])\n",
    "        for child in root:\n",
    "            if child.tag == \"sentence\":\n",
    "                sentence = Sentence(child.attrib['id'], child.attrib['text'])\n",
    "                if len(sentence.text) < 2:\n",
    "                    continue\n",
    "                for second_child in child:\n",
    "                    attr = second_child.attrib\n",
    "                    if second_child.tag == \"entity\":\n",
    "                        entity = Entity(attr['id'], attr['charOffset'], attr['type'], attr['text'])\n",
    "                        sentence.add_entity(entity)\n",
    "                    elif second_child.tag == \"pair\":\n",
    "                        ddi = False\n",
    "                        if attr['ddi'] == \"true\":\n",
    "                            ddi = True\n",
    "\n",
    "                        pair = Pair(attr['id'],attr['e1'],attr['e2'], ddi)\n",
    "                        if pair.ddi and 'type' in attr:\n",
    "                            pair.set_type(attr['type'])\n",
    "\n",
    "                        sentence.add_pair(pair)\n",
    "\n",
    "                document.add_sentence(sentence)\n",
    "        return document\n",
    "\n",
    "    def parse_save_xml_dict(self, xml_dict):\n",
    "        parsed_docs = []\n",
    "        for doc in xml_dict['data']:\n",
    "            print(\"Parsing: \"+doc)\n",
    "            self.set_path(doc)\n",
    "            d = self.parse_xml()\n",
    "            parsed_docs.append(d)\n",
    "\n",
    "        dir_path = abspath(\"data/pickle\")\n",
    "        if not isdir(dir_path):\n",
    "            makedirs(dir_path)\n",
    "\n",
    "        pickle_name = xml_dict['name']+\".pkl\"\n",
    "        with open(join(dir_path, pickle_name),\"wb\") as f:\n",
    "            pickle.dump(parsed_docs, f)\n",
    "            print(\"Saved parsed documents from \" + pickle_name + \" into pickle!\\n\")\n",
    "\n",
    "def parse_all_files():\n",
    "    parser = Parser()\n",
    "    if not exists(\"data/pickle/\"+drug_bank_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_train)\n",
    "    else:\n",
    "        print(\"drug_bank_train already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_train['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_train)\n",
    "    else:\n",
    "        print(\"medline_train already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ddi_test)\n",
    "    else:\n",
    "        print(\"drug_bank_ddi_test already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_ddi_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ddi_test)\n",
    "    else:\n",
    "        print(\"medline_ddi_test already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+drug_bank_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(drug_bank_ner_test)\n",
    "    else:\n",
    "        print(\"drug_bank_ner_test already parsed - skipping\")\n",
    "    if not exists(\"data/pickle/\"+medline_ner_test['name']+\".pkl\"):\n",
    "        parser.parse_save_xml_dict(medline_ner_test)\n",
    "    else:\n",
    "        print(\"medline_ner_test already parsed - skipping\")\n",
    "\n",
    "```\n",
    "\n",
    "### classifier.py =========\n",
    "\n",
    "```python\n",
    "#!/usr/bin/python3\n",
    "from os.path import join, abspath, isdir\n",
    "from os import listdir, makedirs\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "\n",
    "# Files are in the following order:\n",
    "# 0 - medline_ner_test.pkl\n",
    "# 1 - medline_train.pkl\n",
    "# 2 - drug_bank_ddi_test.pkl\n",
    "# 3 - drug_bank_train.pkl\n",
    "# 4 - drug_bank_ner_test.pkl\n",
    "# 5 - medline_ddi_test.pkl\n",
    "\n",
    "pickle_path = \"data/pickle\"\n",
    "pickled_files = [join(abspath(pickle_path), f) for f in listdir(abspath(pickle_path))]\n",
    "\n",
    "class NERClassifier:\n",
    "    def __init__(self):\n",
    "        self.path = \"\"\n",
    "\n",
    "    def set_path(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    # split dataset into classes and sub-dictionaries\n",
    "    # return classes and dictionaries (i.e. feature vectors)\n",
    "    def split_dataset(self):\n",
    "        if len(self.path) == 0:\n",
    "            raise ValueError(\"Path can't be empty\")\n",
    "\n",
    "        with open(self.path, 'rb') as f:\n",
    "            docs = pickle.load(f)\n",
    "\n",
    "        feature_vectors_dict = [] # feature vectors expressed as dicts. train data\n",
    "        classes = [] # B,I,O classes\n",
    "        dict_metadatas = []\n",
    "\n",
    "        for doc in docs:\n",
    "            for m_dict in doc.featured_words_dict:\n",
    "                classes.append(m_dict['0'])\n",
    "                dict_metadatas.append(m_dict['-1'])\n",
    "\n",
    "                # we want sub-dictionary of all elements besides the class\n",
    "                sub_dict = {k:v for k,v in  m_dict.items() if k > '0' and not isinstance(v, list)}\n",
    "                feature_vectors_dict.append(sub_dict)\n",
    "\n",
    "        return (feature_vectors_dict, classes,  dict_metadatas)\n",
    "\n",
    "    # train dataset, where X is a list of feature vectors expressed as dictionary\n",
    "    # and Y is class variable, which is BIO tag in our case\n",
    "    def train_dataset(self, X, Y, kernel):\n",
    "        vec = DictVectorizer(sparse=False)\n",
    "        svm_clf = svm.SVC(kernel = kernel, cache_size = 1800, C = 20, verbose = True, tol = 0.01)\n",
    "        vec_clf = Pipeline([('vectorizer', vec), ('svm', svm_clf)])\n",
    "        vec_clf.fit(X, Y)\n",
    "\n",
    "        return vec_clf\n",
    "\n",
    "    def train_drugbank(self, kernel = 'rbf' ):\n",
    "        self.set_path(pickled_files[3])\n",
    "\n",
    "        X_train, Y_train, metadatas = self.split_dataset()\n",
    "        vec_clf = self.train_dataset(X_train, Y_train, kernel)\n",
    "\n",
    "        if not isdir('models'):\n",
    "            makedirs('models')\n",
    "\n",
    "        model_names = [join(abspath(\"models\"), f) for f in listdir(abspath(\"models\"))]\n",
    "\n",
    "        from operator import contains\n",
    "        drugbank_models = list(filter(lambda x: contains(x, 'drugbank_model_'), model_names))\n",
    "        model_index = len(drugbank_models) # save next model\n",
    "\n",
    "        model_name = 'models/drugbank_model_'+str(model_index)+'.pkl'\n",
    "\n",
    "        joblib.dump(vec_clf, model_name)\n",
    "        print(\"Model trained and saved into\", model_name)\n",
    "\n",
    "    def test_NER_model(self, model_index, test_folder):\n",
    "        model_name = \"\"\n",
    "        predictions_name = \"\"\n",
    "        if test_folder == 1:\n",
    "            model_name = 'models/drugbank_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/drugbank_model_'+str(model_index)+'.txt'\n",
    "            self.set_path(pickled_files[4])\n",
    "        elif test_folder == 2:\n",
    "            model_name = 'models/medline_model_'+str(model_index)+'.pkl'\n",
    "            predictions_name = 'predictions/medline_model_'+str(model_index)+'.txt'\n",
    "            self.set_path(pickled_files[0])\n",
    "        else:\n",
    "            raise ValueError('test_folder value should be 1 - drugbank, or 2 - medline')\n",
    "\n",
    "        print(\"Testing model\", model_index,\"...\")\n",
    "\n",
    "        vec_clf = joblib.load(model_name)\n",
    "\n",
    "        # metadatas are of type: sentenceId | offsets... | text | type\n",
    "        X_test, Y_test, metadatas = self.split_dataset()\n",
    "        predictions = vec_clf.predict(X_test)\n",
    "        assert len(predictions) == len(Y_test) == len(metadatas)\n",
    "\n",
    "        if not isdir('predictions'):\n",
    "            makedirs('predictions')\n",
    "\n",
    "        predictions_name = 'predictions/drugbank_model_'+str(model_index)+'.txt'\n",
    "        pr_f = open(predictions_name,'w')\n",
    "        # clear file, i.e. remove all\n",
    "        pr_f.close()\n",
    "\n",
    "        # reopen clean file\n",
    "        pr_f = open(predictions_name, 'w')\n",
    "\n",
    "        for i, pred in enumerate(predictions):\n",
    "            metadata = metadatas[i]\n",
    "\n",
    "            # if prediction is B_type or I_type then we predicted the drug and it's type is after B_\n",
    "            if pred[:2] == 'B_' or pred[:2] == 'I_':\n",
    "                line = metadata[0] + '|' + metadata[1] + '|' + metadata[2] + '|' + pred[2:]\n",
    "                pr_f.write(line + '\\n')\n",
    "\n",
    "        print(\"Predictions are saved in file\", predictions_name)\n",
    "        pr_f.close()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
