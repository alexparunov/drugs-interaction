{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT Term Project - DDI Classifier\n",
    "## Alex Paranov, Anthony Nixon\n",
    "### MIRI Masters - Term 2 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining Python classes for XML processing\n",
    "\n",
    "The first component of our project is to create data structures in which we can store and manipulate the xml data in an effecient manner. This code is available in xml_classes.py (see appendix for source).\n",
    "\n",
    "We create four classes which correspond to the tagged elements in the xml annotation:\n",
    "\n",
    "#### Document:\n",
    "Represents and stores a full text sample consisting of sentence objects. The document class also contains a function \"set_features()\" which passes a call to a set_features() method at the sentence level and assigns featured words to the document featured_words list.\n",
    "\n",
    "#### Sentence:\n",
    "A sentence is a discrete segment of text which can be broken down into entities and pairs. The composing entities and pairs are stored in lists in the object of the same name.\n",
    "\n",
    "An important part of the Sentence class is it's \"set_features()\" function which, along with it’s helper functions, iterates over the entities and splits the words in the text and tags them. For each tagged_word the helper function \"get_featured_tuple()\" returns a list of features based on orthographic features, prefix and suffix, word shapes, etc. (We will cover these features and their rationale in more detail in the Part 3: feature extraction section of this report).\n",
    "\n",
    "#### Entity:\n",
    "Stores a relevent mention of a drug name / substance / etc. in a sentence as well as the location offset.\n",
    "\n",
    "#### Pair:\n",
    "A pair is a drug drug interation relating entities in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to import the classes\n",
    "from xml_classes import Document, Sentence, Entity, Pair\n",
    "print(\"DSEP classes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing\n",
    "\n",
    "After we have built the structures to store our data, we parse the data. Our parsing code is contained in parser.py (see appendix for source). The primary execution is initiated by the parse_all_files() method. The parser first looks to see if the project files have been previously parsed and stored locally, if not, then it will begin parsing.\n",
    "\n",
    "The parser stores the data in our Document, Sentence, Entity, and Pair objects. It then takes these objects and writes them to local disk as pickle files. In this manner, we only have to run the parser once.\n",
    "\n",
    "The data parsed is the drug_bank and med_line training sets as well as the drug-drug-interaction and name-entity-recognition test sets for both drug_bank and med_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to begin parsing\n",
    "from parser import main as parse_all_files\n",
    "parse_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction\n",
    "\n",
    "The function extract_features() below loads the parsed pickle objects back into memory and calls set_features() on the documents.\n",
    "\n",
    "We follow an object oriented paradigm where each sentence object in a of the document object returns all features of its composing text (see appndix xml_classes.py for source).\n",
    "\n",
    "For our feature vectors we include the following features:\n",
    "\n",
    "- BIO (beginning, inside, outside) tag\n",
    "- Word windows consisting of [word, pos_tag] for +- 2 words\n",
    "- Boolean of length >= 7\n",
    "- Orthographical features: alphanumeric, capitalization, digits, hyphenation\n",
    "- Whether prefix and suffix of words are length 3, 4, or 5 boolean e.g. [pl3, pl4, pl5, sufl3, sufl4, sufl5]\n",
    "- Word-shape:\n",
    "    1. Generalized word shape, where the pattern of alphanumeric (X = upper, x = lower, 0 = numeric, O = other) eg. Aspirin1+ maps to Xxxxxxx00.\n",
    "\n",
    "    2. Brief word shape, where consecutive forms aren’t condensed. eg. Aspirin1+ maps to Xx0O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cell to load documents from pickle file and extract features from\n",
    "# the sentences.\n",
    "\n",
    "from os.path import join, abspath\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "pickle_path = \"data/pickle\"\n",
    "pickled_files = [join(abspath(pickle_path), f) for f in listdir(abspath(pickle_path))]\n",
    "\n",
    "def extract_features():\n",
    "    for file_name in pickled_files:\n",
    "        f = open(file_name, 'rb')\n",
    "        docs = pickle.load(f)\n",
    "        f.close()\n",
    "        all_featured_docs = []\n",
    "        for doc in docs:\n",
    "            #print(\"Extracting features for\",doc.id)\n",
    "            doc.set_features()\n",
    "            all_featured_docs.append(doc)\n",
    "\n",
    "        with open(file_name,'wb') as f:\n",
    "            pickle.dump(all_featured_docs, f)\n",
    "            print(\"All documents with features are set in \"+file_name)\n",
    "\n",
    "extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part 4: Classification for Name-Entity-Recognition\n",
    "\n",
    "Our classification takes place through the NERClassifier class. The primary functions of the class are train_drugbank() and test_NER_model() which correspond to training and testing of the name entity recognition task.\n",
    "\n",
    "The functions handle the loading of the data, one-hot-encoding, parameters and call to the SVM classifier and the result output.\n",
    "We experimented with both an RBF and linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cell to construct classifier\n",
    "\n",
    "from classifier import NERClassifier\n",
    "import warnings\n",
    "\n",
    "nerCl = NERClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN A MODEL - based on feature vectures extracted in Part 3 - Run cell below. If you would like to use a pre-existing model, skip to next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supress scikit warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    nerCl.train_drugbank(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST A MODEL - run the cell below with relevant parameters, model_index (you will find this appended to the file name of the model) and test_folder (1 = drugbank, 2 = medline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerCl.test_NER_model(model_index = 2, test_folder = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
